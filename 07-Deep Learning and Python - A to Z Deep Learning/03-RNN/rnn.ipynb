{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Sequence Models](#1)\n",
    "* [Recurrent Neural Network (RNN)](#2)\n",
    "* [Implementing Recurrent Neural Network with Keras](#3)\n",
    "    * [Loading and Preprocessing Data](#31)\n",
    "    * [Create RNN Model](#32)\n",
    "    * [Predictions and Visualising RNN Model](#33)\n",
    "* [Long Short Term Memory (LSTMs)](#4)\n",
    "* [Implementing Long Short Term Memory with Keras](#99)\n",
    "    * [Loading and Visualizing Data](#41)\n",
    "    * [Preprocessing Data](#42)\n",
    "    * [Create LSTM Model](#43)\n",
    "    * [Predictions and Visualising LSTM Model](#44)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Models\n",
    "* Sequence models plays an over time. \n",
    "* Speech recognition, natural language process (NLP), music generation\n",
    "* Apples Siri and Google's voice search\n",
    "* Sentiment classification (duygu sınıflandırma) Mesela \"bu ders bu dunyadaki en guzel ders\" yada \"sacma sapan ders cekmissin hocaaa\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network\n",
    "* RNN’s are able to remember important things about the input they received, which enables them to be very precise in predicting what’s coming next.\n",
    "* This is the reason why they are the preferred algorithm for sequential data like time series, speech, text, financial data, audio, video, weather and much more because they can form a much deeper understanding of a sequence and its context, compared to other algorithms.\n",
    "* Not only feeds output but also gives feed backs into itself. Because RNN has internal memory\n",
    "* temporal loop = zamansal döngü. Kendini besler.\n",
    "* Belleğe sahipler short term memory bir önceki node da olanları hatırlarlar. Eskiyi hatırlar.\n",
    "* Mesela geçmişi hatırlamak neden önemli biz yaptıklarımızdan bir şeyler öğreniriz ve yeni öğrenilen şeyleri de eski öğrendiklerimizi üzerine kurarız. RNN'i de aynı mantıkta düşünebilirsiniz. Film örneğinde olduğu gibi.\n",
    "* Örnek RNN yapılarına bakalım\n",
    "* One to Many\n",
    "    * Input bir resim output o resimde yapılan cümle yani \"Adam surf yapıyor\"\n",
    "* Many to One\n",
    "    * Input bir cümle output bir duygu mesela iyimser neşeli gibi.\n",
    "* Many to Many\n",
    "    * Mesela google translate kullanarak İngilizceden bir cümleyi Türkçe'ye translate etmek\n",
    "* RNN short term memory'e sahip ama LSTM long term memory'e de sahip olabiliyor.\n",
    "* RNN'i ANN yada CNN'den ayıran daha önce de belirttiğimiz gibi *memory*. Mesela \"DATAI\" diye bir stringimiz var ve biz 4. harfe geldik yani \"A\" harfine. ANN' e sorduğumuz zaman 4. harfi A olan bir kelimenin 5. harfi ne olabilir diye. ANN bilemez çünkü memory olmadığı için geçmiş harfleri yani \"DAT\" harflerini bilip \"A\" ile birleştirip daha sonra 5. harf \"I\" olabilir diyemez. Ama RNN tam olarak bunu söyleyebilir.\n",
    "* Exploiding Gradients: Gradient'in çok büyük olması durumu. Gereksiz yere belli weightlere önem kazandırır.\n",
    "* Vanishing Gradients: Gradient'in çok küçük olması durumu. Yavaş öğrenir.\n",
    "* Gradient neydi arkadaşlar costa göre weightlerde ki değişim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Recurrent Neural Network with Keras\n",
    "* [Loading and Preprocessing Data](#31)\n",
    "* [Create RNN Model](#32)\n",
    "* [Predictions and Visualising RNN Model](#33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Read Data and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = pd.read_csv('data/Stock_Price_Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "Bu verisetinde yalnızca Open feature'ı üzerinde çalışalıcaktır. Amaç RNN öğrenmektir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset_train.loc[:, [\"Open\"]].to_numpy()\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "scaler = MinMaxScaler(feature_range = (0, 1)) # min-max normalization\n",
    "train_scaled = scaler.fit_transform(train)\n",
    "train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_scaled)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "Burada 50 sample alacağız(x_train), sonraki indexi mesela 51. yi predict edeceğiz, daha sonra 1 index sonraya shifting edip aynı işlem yapılacaktır. Örneğin 3 sample için:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/timestep.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data structure with 50 timesteps and 1 output\n",
    "X_train = []\n",
    "y_train = []\n",
    "timesteps = 50\n",
    "for i in range(timesteps, 1258):\n",
    "    X_train.append(train_scaled[i-timesteps:i, 0])\n",
    "    y_train.append(train_scaled[i, 0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Create RNN Model and Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the RNN\n",
    "regressor = Sequential()\n",
    "\n",
    "# Adding the first RNN layer and some Dropout regularisation\n",
    "regressor.add(SimpleRNN(units = 50, activation='tanh', return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a second RNN layer and some Dropout regularisation\n",
    "regressor.add(SimpleRNN(units = 50, activation='tanh', return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a third RNN layer and some Dropout regularisation\n",
    "regressor.add(SimpleRNN(units = 50, activation='tanh', return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a fourth RNN layer and some Dropout regularisation\n",
    "regressor.add(SimpleRNN(units = 50))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding the output layer\n",
    "regressor.add(Dense(units = 1))\n",
    "\n",
    "# Compiling the RNN\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "# Fitting the RNN to the Training set\n",
    "regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-Predict (Test) Data and Visualising RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the real stock price of 2017\n",
    "dataset_test = pd.read_csv('data/Stock_Price_Test.csv')\n",
    "dataset_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_stock_price = dataset_test.loc[:, [\"Open\"]].to_numpy()\n",
    "real_stock_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted stock price of 2017\n",
    "dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0)\n",
    "inputs = dataset_total[len(dataset_total) - len(dataset_test) - timesteps:].values.reshape(-1,1)\n",
    "inputs = scaler.transform(inputs)  # min max scaler\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data\n",
    "X_test = []\n",
    "for i in range(timesteps, 70):\n",
    "    X_test.append(inputs[i-timesteps:i, 0])\n",
    "X_test = np.array(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "predicted_stock_price = regressor.predict(X_test)\n",
    "predicted_stock_price = scaler.inverse_transform(predicted_stock_price) # scale edilmiş değerleri, gerçek değerlere dönüştürür."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the results\n",
    "plt.plot(real_stock_price, color = 'red', label = 'Real Google Stock Price')\n",
    "plt.plot(predicted_stock_price, color = 'blue', label = 'Predicted Google Stock Price')\n",
    "plt.title('Google Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Google Stock Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# epoch = 250 daha güzel sonuç veriyor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B-Long Short Term Memory (LSTMs)\n",
    "* LSTM is variant of RNN.\n",
    "* LSTM de RNN'den farklı olarak long term memory var. \n",
    "* LSTM architecture:\n",
    "    * x: scaling of information\n",
    "    * +: Adding information\n",
    "    * sigmoid layer. Sigmoid memory den bir şeyi hatırlamak için yada unutmak için kullanılır. 1 yada 0'dır.\n",
    "    * tanh: activation function tanh. Tanh vanishing gradient(yavaş öğrenme - çok küçük gradient) problemini çözer. Çünkü parametreleri update ederken türev alıyorduk. Tanh'ın türevi hemen sıfır'a ulaşmaz.\n",
    "    * h(t-1): output of LSTM unit\n",
    "    * c(t-1): memory from previous LSTM unit\n",
    "    * X(t): input\n",
    "    * c(t): new updated memory\n",
    "    * h(t): output\n",
    "    * From c(t-1) to c(t) is memory pipeline. or only memory.\n",
    "    * Oklar vektör.\n",
    "    * h(t-1) ile X(t) birleşmiyor parallel iki yol olarak düşünebilirsiniz.\n",
    "* <img src=\"img/lstm.jpg\">\n",
    "* 1) Forget gate: input olarak X(t) ve h(t-1) alır. Gelen bilginin unutulup unutulmayacağına karar verir.\n",
    "* 2) Input gate: Hangi bilginin memory de depolanıp depolanmayacağına karar verir.\n",
    "* 3) Output gate: Hangi bilginin output olup olmayacağına karar verir.\n",
    "* Örneğin: \n",
    "    * ... \"Boys are watching TV\"\n",
    "    * \"On the other hand girls are playing baseball.\"\n",
    "    * Forget \"boys\". new input is \"girls\" and output is \"girls\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"99\"></a>\n",
    "## Implementing Long Short Term Memory with Keras\n",
    "* [Loading and Visualizing Data](#41)\n",
    "* [Preprocessing Data](#42)\n",
    "* [Create LSTM Model](#43)\n",
    "* [Predictions and Visualising LSTM Model](#44)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/passengers.csv\")\n",
    "data.head() # İlk indexte 1 ayda international airline'dan 112 kişi geçmiş."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.iloc[:, 1].to_numpy()\n",
    "plt.plot(dataset)\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"Number of Passenger\")\n",
    "plt.title(\"international airline passenger\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Preprocessing Data\n",
    "* reshape\n",
    "* change type\n",
    "* scaling\n",
    "* train test split\n",
    "* Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.reshape(-1, 1) # (142,) => ( 142,1)\n",
    "dataset = dataset.astype(\"float32\")\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling \n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(dataset) * 0.50)\n",
    "test_size = len(dataset) - train_size\n",
    "train = dataset[0:train_size,:]\n",
    "test = dataset[train_size:len(dataset),:]\n",
    "print(\"train size: {}, test size: {} \".format(len(train), len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_stemp = 10\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(len(train)-time_stemp-1):\n",
    "    a = train[i:(i+time_stemp), 0]\n",
    "    dataX.append(a)\n",
    "    dataY.append(train[i + time_stemp, 0])\n",
    "trainX = np.array(dataX)\n",
    "trainY = np.array(dataY)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(len(test)-time_stemp-1):\n",
    "    a = test[i:(i+time_stemp), 0]\n",
    "    dataX.append(a)\n",
    "    dataY.append(test[i + time_stemp, 0])\n",
    "testX = np.array(dataX)\n",
    "testY = np.array(dataY)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-Create LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = Sequential()\n",
    "model.add(LSTM(10, input_shape=(1, time_stemp))) # 10 lstm neuron(block)\n",
    "model.add(Dense(1)) #activation function LSTM'de gerek yok \n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=50, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-Predictions and Visualising LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict) # original value\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shifting train\n",
    "trainPredictPlot = np.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[time_stemp:len(trainPredict)+time_stemp, :] = trainPredict\n",
    "# shifting test predictions for plotting\n",
    "testPredictPlot = np.empty_like(dataset)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(trainPredict)+(time_stemp*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and pr edictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
